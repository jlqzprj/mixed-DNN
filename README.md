# mixed-DNN
Python codes for reproducing experiements in the paper https://arxiv.org/abs/2303.16454.


1. Visualization:

   The prediction/approximation of the conductivity q at selected prediction points is saved in 'YOUR_OUTPUT_FILE_NAME.mat' by the command line:
   
   'scipy.io.savemat('YOUR_OUTPUT_FILE_NAME.mat', {'sigma': sigpred})'
   
   at the end of the code. 


   The plots (exact q, learned q, and pointwise error) shown in the paper can be generated by running 'visualization.m' (with suitable changes, e.g., your own     path / file name) in Matlab.



2. Initialization:

   The loss function 'loss' is designed according to formulations (3.6 & 3.7) and (4.4 & 4.5) in the paper.

   The projection operator P_A requires good initialization of the network q_theta (i.e., c_0 <= q_theta <= c_1), otherwise the piecewise constancy of the    projection function will cause problems during the optimization process.

   To ensure the network q_theta is initialized well, one can either

   a) Choose different initialization methods for weights and biases;

   or

   b)
   i) Run the script without the projection operator (i.e., replace 'self.sigprj' with 'self.sig' in 'self.loss') for a few epochs (200 in our case) until q_theta       satisfies the inequality c_0 <= q_theta <= c_1.

      Uncomment the lines 
      'saver = tf.train.Saver(tf.trainable_variables())'
      and
      'saver.save(self.sess, os.path.join('YOUR_PATH', 'YOUR_VARIABLE_STORE_FILE_NAME.ckpt'))'
      in the train section to save values of the trainable variables (i.e., weights and biases).







